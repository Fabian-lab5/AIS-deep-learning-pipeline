{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (papermill will inject these)\n",
    "augmentation_type = \"default\"  # Default value, overridden by papermill\n",
    "seed = -1      # Default value, overridden by papermill\n",
    "num_aug = -1  # Default value, overridden by papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Dropout, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "from io import BytesIO, StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_seed = 3141\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "random.seed(comparison_seed)\n",
    "np.random.seed(comparison_seed)\n",
    "tf.random.set_seed(comparison_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df):\n",
    "    display(df)\n",
    "    \n",
    "    unique_vessel_groups = df['VesselGroup'].unique()\n",
    "    num_vessel_groups = df['VesselGroup'].nunique()\n",
    "    \n",
    "    num_rows = f\"{len(df):,}\"\n",
    "    \n",
    "    print(f\"The dataframe contains {num_rows} rows.\")\n",
    "    print()\n",
    "    print(f\"There are {num_vessel_groups} unique vessel groups.\")\n",
    "    print(f\"The unique vessel groups are: {unique_vessel_groups}\")\n",
    "    \n",
    "    print(\"\\nRow counts for each vessel group:\")\n",
    "    for group in unique_vessel_groups:\n",
    "        group_count = len(df[df['VesselGroup'] == group])\n",
    "        print(f\"Group {group}: {group_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 40\n",
    "step_size = 1 \n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation_type == \"baseline\":\n",
    "    run_name = f\"baseline_{seed}\"\n",
    "else:\n",
    "    run_name = f\"{augmentation_type}_aug{num_aug}_{seed}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation_type == \"baseline\" and num_aug == 0:\n",
    "    file_path = f\"/Users/fabian/Downloads/MasterDegree/df_train_{seed}.csv\"\n",
    "    print(file_path)\n",
    "else:\n",
    "    folder_name = augmentation_type.replace(\" \", \"_\")\n",
    "    \n",
    "    if augmentation_type == \"GNI\":\n",
    "        file_prefix = \"GNI\"\n",
    "    elif augmentation_type == \"vae\":\n",
    "        file_prefix = \"vae\"\n",
    "    elif augmentation_type == \"kmeans\":\n",
    "        file_prefix = \"kmeans\"\n",
    "\n",
    "    file_path = f\"/Users/fabian/Downloads/MasterDegree/{file_prefix}/{file_prefix}{seed}/df_{file_prefix}_{num_aug}.csv\"\n",
    "    print(file_path)\n",
    "\n",
    "df_train = pd.read_csv(file_path)\n",
    "df_test = pd.read_csv(f\"/Users/fabian/Downloads/MasterDegree/df_test_{seed}.csv\")\n",
    "df_val = pd.read_csv(f\"/Users/fabian/Downloads/MasterDegree/df_val_{seed}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n===== Train Data =====\")\n",
    "summarize_dataframe(df_train)\n",
    "\n",
    "print(f\"\\n===== Test Data =====\")\n",
    "summarize_dataframe(df_test)\n",
    "\n",
    "print(f\"\\n===== Validation Data =====\")\n",
    "summarize_dataframe(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(df_train)\n",
    "test_size = len(df_test)\n",
    "val_size = len(df_val)\n",
    "\n",
    "rows_sum = train_size + test_size + val_size\n",
    "\n",
    "print(f\"The DataFrame (without augmentation) contains {rows_sum} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['MMSI'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df_train['MMSI']).intersection(df_test['MMSI']))\n",
    "print(set(df_train['MMSI']).intersection(df_val['MMSI']))\n",
    "print(set(df_val['MMSI']).intersection(df_test['MMSI']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data, labels, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Create sliding windows from data and labels.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.array): The feature array of shape (num_samples, num_features).\n",
    "        labels (np.array): The label array (one-hot encoded) of shape (num_samples, num_classes).\n",
    "        window_size (int): The size of each sliding window.\n",
    "        step_size (int): The step size for the sliding window.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The sliding window feature array of shape (num_windows, window_size, num_features).\n",
    "        np.array: The sliding window label array of shape (num_windows, num_classes).\n",
    "    \"\"\"\n",
    "    if len(data) != len(labels):\n",
    "        raise ValueError(\"Data and labels must have the same length.\")\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(labels[i + window_size - 1])  \n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"df_class.csv\", index=False)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Final VG Pred Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['LAT', 'LON', 'SOG', 'COG']\n",
    "target_column = 'VesselGroup'\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Dataset'] = 'train'\n",
    "df_val['Dataset'] = 'val'\n",
    "df_test['Dataset'] = 'test'\n",
    "\n",
    "df_combined = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_combined[feature_columns] = scaler.fit_transform(df_combined[feature_columns])\n",
    "display(df_combined)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_combined['VesselGroup_encoded'] = label_encoder.fit_transform(df_combined[target_column])\n",
    "\n",
    "df_train_scaled = df_combined[df_combined['Dataset'] == 'train'].drop(columns=['Dataset'])\n",
    "df_val_scaled = df_combined[df_combined['Dataset'] == 'val'].drop(columns=['Dataset'])\n",
    "df_test_scaled = df_combined[df_combined['Dataset'] == 'test'].drop(columns=['Dataset'])\n",
    "\n",
    "train_features = df_train_scaled[feature_columns ].values\n",
    "train_labels = df_train_scaled['VesselGroup_encoded'].values\n",
    "\n",
    "val_features = df_val_scaled[feature_columns ].values\n",
    "val_labels = df_val_scaled['VesselGroup_encoded'].values\n",
    "\n",
    "test_features = df_test_scaled[feature_columns].values\n",
    "test_labels = df_test_scaled['VesselGroup_encoded'].values\n",
    "\n",
    "print(\"Classes in LabelEncoder:\", label_encoder.classes_)\n",
    "print(\"Training features shape:\", train_features.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Validation features shape:\", val_features.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "print(class_names)\n",
    "\n",
    "y_train = to_categorical(train_labels, num_classes=num_classes)\n",
    "y_val = to_categorical(val_labels, num_classes=num_classes)\n",
    "y_test = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "print(\"y_train shape after one-hot encoding:\", y_train.shape)\n",
    "print(\"y_val shape after one-hot encoding:\", y_val.shape)\n",
    "print(\"y_test shape after one-hot encoding:\", y_test.shape)\n",
    "\n",
    "print(\"Unique vectors in y_train:\", np.unique(y_train, axis=0))\n",
    "print(\"y_train shape after one-hot encoding:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_sliding_windows(train_features, y_train, window_size, step_size)\n",
    "X_val, y_val = create_sliding_windows(val_features, y_val, window_size, step_size)\n",
    "X_test, y_test = create_sliding_windows(test_features, y_test, window_size, step_size)\n",
    "\n",
    "print(\"Sliding window shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "unique, counts = np.unique(y_train_labels, return_counts=True)\n",
    "print(\"Class distribution in y_train:\", dict(zip(unique, counts)))\n",
    "\n",
    "y_val_labels = np.argmax(y_val, axis=1)\n",
    "unique, counts = np.unique(y_val_labels, return_counts=True)\n",
    "print(\"Class distribution in y_val:\", dict(zip(unique, counts)))\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "unique, counts = np.unique(y_test_labels, return_counts=True)\n",
    "print(\"Class distribution in y_test:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "    y=np.argmax(y_train, axis=1)\n",
    ")\n",
    "\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"Class weights for baseline model: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(LSTM(lstm_units, return_sequences=False, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(dropout_rate)) \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = model.to_json()\n",
    "file_path = \"model_architecture.json\"\n",
    "\n",
    "with open(\"model_architecture.json\", \"w\") as f:\n",
    "    f.write(model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow experiment run\n",
    "with mlflow.start_run(run_name = run_name) as run:\n",
    "    # Log artifacts and model parameters to MLflow\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"window_size\", window_size)\n",
    "    mlflow.log_param(\"step size\", step_size)\n",
    "    mlflow.log_param(\"lstm_units\", lstm_units)\n",
    "    mlflow.log_param(\"num_classes\", num_classes)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "    mlflow.log_param(\"train_size\", train_size)\n",
    "    mlflow.log_param(\"test_size\", test_size)\n",
    "    mlflow.log_param(\"val_size\", val_size)\n",
    "    mlflow.log_param(\"rows_sum\", rows_sum)\n",
    "    mlflow.log_param(\"num_aug\", num_aug)\n",
    "    mlflow.log_param(\"aug_type\", augmentation_type)\n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "\n",
    "    mlflow.log_artifact(file_path)\n",
    "    mlflow.log_artifact(\"df_class.csv\")\n",
    "\n",
    "    mlflow.keras.log_model(model, \"class_lstm_baseline\")\n",
    "    \n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Model - Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    \n",
    "\n",
    "    for epoch, acc in enumerate(history.history['accuracy']):\n",
    "        mlflow.log_metric(\"training_accuracy\", acc, step=epoch)\n",
    "    for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "        mlflow.log_metric(\"validation_accuracy\", val_acc, step=epoch)\n",
    "    for epoch, loss in enumerate(history.history['loss']):\n",
    "        mlflow.log_metric(\"training_loss\", loss, step=epoch)\n",
    "    for epoch, val_loss in enumerate(history.history['val_loss']):\n",
    "        mlflow.log_metric(\"validation_loss\", val_loss, step=epoch)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "os.remove(\"df_class.csv\")\n",
    "os.remove(\"model_architecture.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "y_pred_labels = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "y_test_labels = y_test.argmax(axis=1)\n",
    "\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "\n",
    "print(f'F1 Score (Weighted): {f1:.4f}')\n",
    "print(f'Precision (Weighted): {precision:.4f}')\n",
    "print(f'Recall (Weighted): {recall:.4f}')\n",
    "\n",
    "report = classification_report(y_test_labels, y_pred_labels, target_names=class_names, output_dict=True)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_labels, y_pred_labels, target_names=class_names))\n",
    "\n",
    "for class_name in class_names:\n",
    "    f1_score_key = f\"test_f1_{class_name}\"\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "for i in range(10):\n",
    "    true_label = class_names[y_test_labels[i]]\n",
    "    predicted_label = class_names[y_pred_labels[i]]\n",
    "    print(f\"Sample {i}: True Class = {true_label}, Predicted Class = {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "unique_classes = np.unique(y_pred_labels)\n",
    "num_unique_classes = len(unique_classes)\n",
    "\n",
    "print(f\"Unique Predicted Classes: {unique_classes}\")\n",
    "print(f\"Number of Unique Predicted Classes: {num_unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history.get('accuracy')\n",
    "val_acc = history.history.get('val_accuracy')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if train_acc and val_acc:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
